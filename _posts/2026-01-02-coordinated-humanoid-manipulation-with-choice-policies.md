---
title: "协调型人形机器人操作与选择策略"
date: 2026-01-02 16:04:04 +0800
arxiv_id: 2512.25072v1
---

## 论文信息

**标题**: Coordinated Humanoid Manipulation with Choice Policies

**作者**: Haozhi Qi, Yen-Jen Wang, Toru Lin, et al.

**发布日期**: 2025-12-31

**arXiv ID**: [2512.25072v1](https://arxiv.org/abs/2512.25072v1)

**PDF链接**: [下载PDF](https://arxiv.org/pdf/2512.25072v1)

---


# 从模块化遥操作到选择策略：人形机器人全身协调操控的新范式

## 引言：人形机器人的协调操控难题

人形机器人因其与人类环境的天然兼容性，被视为未来服务机器人、工业助手乃至家庭伙伴的理想形态。然而，要实现其在非结构化环境中的实际应用，必须解决一个根本性挑战：**如何实现头、手、腿的全身协调操控**。传统方法往往将操控任务分解为独立的子问题（如导航、抓取、视觉伺服），但缺乏一个统一的框架来整合这些模块，导致机器人在执行复杂、长时程任务时表现僵硬、易失败。

近期，来自顶尖机器人实验室的研究团队在论文《Coordinated Humanoid Manipulation with Choice Policies》中提出了一套创新解决方案。该研究不仅设计了一个**模块化遥操作接口**来高效收集演示数据，还引入了一种名为**选择策略（Choice Policy）** 的模仿学习架构，显著提升了人形机器人在复杂操控任务中的表现。本文将从技术背景、核心方法、实验结果及实践启示等多个维度，深入解析这项工作的突破性贡献。

## 一、 研究背景与核心动机

### 1.1 人形机器人操控的“数据困境”
当前，数据驱动的模仿学习（Imitation Learning）和强化学习（Reinforcement Learning）是解决复杂机器人操控的主流方向。然而，对于人形机器人而言，获取高质量的训练数据异常困难：
*   **动作空间维度高**：人形机器人具有数十个自由度（DoF），直接进行全身遥操作对操作员要求极高，且数据效率低下。
*   **行为模式多峰性（Multimodality）**：同一任务目标（如“拿起杯子”）可以通过多种不同的动作序列实现。传统的行为克隆（Behavior Cloning）方法假设数据分布是单峰的，容易导致“平均化”动作，在关键决策点（如选择抓取点）上失效。
*   **长时程依赖**：像“将碗放入洗碗机”这样的任务，涉及移动、视觉定位、抓取、放置等一系列子步骤，需要模型具备长时程的规划和协调能力。

### 1.2 现有方法的局限
*   **扩散策略（Diffusion Policies）**：近年来兴起的扩散模型在生成多峰行为数据方面表现出色，但其迭代去噪的推理过程较慢，可能无法满足实时机器人控制的需求。
*   **标准行为克隆（BC）**：简单高效，但无法处理多峰性，在分布外（OOD）状态下容易失效。
*   **强化学习（RL）**：虽能通过试错探索最优策略，但在真实机器人上进行训练成本高昂、风险大，且样本效率低。

**因此，本研究的核心动机是：构建一个既能高效收集高质量人形机器人演示数据，又能学习鲁棒、多峰、快速推理策略的端到端框架。**

## 二、 核心方法：模块化遥操作与选择策略

该研究提出的系统是一个“数据收集-策略学习”的闭环，其核心创新在于前后两端。

### 2.1 前端：模块化遥操作接口
研究团队没有采用直接映射操作员全身动作的“化身”式遥操作，而是将人形机器人的控制**分解为一系列符合人类直觉的子模块**：
1.  **手眼协调（Hand-Eye Coordination）**：操作员通过VR控制器或鼠标直接控制机器人“注视点”和手部目标位置，系统自动解算头部（相机）和手臂的协同运动。
2.  **抓取原语（Grasp Primitives）**：预定义一系列抓握手势（如捏取、抓握），操作员只需触发指令，机器人手部自动执行。
3.  **手臂末端执行器跟踪（Arm End-Effector Tracking）**：操作员可精细控制单只手臂末端（手腕）的6D位姿。
4.  **移动（Locomotion）**：通过摇杆或键盘控制机器人基座（双脚）的移动。

**技术优势**：
*   **降低操作门槛**：操作员无需精通机器人运动学，只需关注高级任务目标。
*   **提升数据质量与效率**：模块化避免了不自然或不可行的动作，收集到的演示数据直接适用于机器人本体，且收集速度更快。
*   **自然解耦任务**：这种分解方式恰好对应了复杂任务中的子技能，为后续策略学习提供了结构化的数据基础。

### 2.2 后端：选择策略学习架构
这是本文在算法层面的主要创新。选择策略的核心思想是：**先生成多个合理的候选动作，再学习一个评分器从中选出最佳动作。**

其网络架构如下图所示（概念示意）：
```
观测历史 (o_t) --> 候选动作生成器 (Generator) --> N个候选动作 {a_t^1, a_t^2, ..., a_t^N}
                                     |
                                     --> 评分器 (Scorer) --> 每个动作的得分 {s^1, s^2, ..., s^N}
最终动作 = argmax(得分) 对应的候选动作
```
*   **候选动作生成器**：通常是一个轻量化的神经网络（如MLP），接收当前及历史的观测（图像、关节状态等），输出N个可能的动作向量。它负责覆盖动作空间中的多峰分布。
*   **评分器**：另一个神经网络，接收相同的观测和每一个候选动作，预测执行该动作后未来任务成功的概率（或Q值）。它负责评估和选择。

**技术优势**：
*   **显式处理多峰性**：生成器直接输出多个选项，避免了行为克隆的“平均效应”。
*   **快速推理**：相比扩散策略需要数十步迭代，选择策略只需一次前向传播生成候选，再加一次评分前向传播（可并行），速度更快。
*   **可解释性**：我们可以分析被拒绝的候选动作，理解策略的决策过程。
*   **兼容离线学习**：整个框架可以完全基于离线收集的演示数据进行训练（离线强化学习/模仿学习），无需危险的环境交互。

## 三、 实验验证与结果分析

研究团队在两个真实的复杂任务上验证了系统有效性：
1.  **洗碗机装载**：要求人形机器人打开洗碗机门，从台面上拿起碗，并将其放入洗碗机篮内。这是一个典型的长时程、精调操作任务。
2.  **全身移动操控-擦白板**：机器人需要走到白板前，拿起板擦，擦拭指定区域。此任务综合了移动、手眼协调和力控交互。

### 3.1 对比基线
*   **标准行为克隆**
*   **扩散策略**（当前最先进的多峰模仿学习方法）

### 3.2 关键结果
1.  **性能领先**：在两项任务的成功率上，**选择策略均显著优于扩散策略和行为克隆**。特别是在洗碗机任务中，选择策略的成功率接近80%，而扩散策略约为60%，行为克隆则低于40%。这证明了其在长时程、多步骤任务中的优越性。
2.  **手眼协调的关键性**：通过消融实验，研究者发现，如果禁用或削弱手眼协调模块，所有方法的性能都大幅下降。这定量验证了**主动视觉感知对于复杂物体操控是不可或缺的**——机器人需要主动移动头部和相机来更好地定位和跟踪目标。
3.  **推理速度**：选择策略的推理速度比扩散策略快一个数量级，能满足实时控制（>10Hz）的要求。
4.  **数据效率**：得益于高质量的模块化演示，系统仅需数百条演示轨迹就能学习到有效的策略，展现了较高的数据效率。

## 四、 实践应用建议与未来方向

### 4.1 对机器人开发者的启示
*   **任务分解与模块化设计**：在开发复杂机器人应用时，不应盲目追求端到端的“黑箱”模型。借鉴本文思想，将任务**自上而下分解为符合认知的子模块**（导航、视觉搜索、精细操作等），并设计相应的接口或子策略，可以大幅降低系统开发难度和数据处理复杂度。
*   **仿真-实迁的数据管道**：模块化遥操作接口可以无缝应用于仿真环境。开发者可以在高保真仿真中高效收集海量演示数据，训练初步策略，再通过少量真实数据微调（Sim-to-Real）。这为规模化机器人学习提供了可行路径。
*   **评估多峰性处理能力**：在评估模仿学习或离线强化学习策略时，应设计包含多峰决策点的测试任务，并关注模型在分布外状态下的鲁棒性。选择策略提供了一种有效的架构参考。

### 4.2 未来研究方向
1.  **生成器与评分器的协同优化**：目前生成器和评分器是分开训练的。未来可以探索端到端的训练方式，让生成器学会产生更易于评分器区分优劣的候选动作。
2.  **与大型基础模型结合**：将视觉-语言基础模型（VLMs）或大语言模型（LLMs）集成到系统中。例如，用LLM理解高级任务指令并规划子模块调用序列，用VLM增强场景理解和物体识别能力。
3.  **从模仿到自主探索**：当前框架完全依赖于人类演示。下一步可以引入**离线强化学习**，利用收集到的演示数据作为初始策略，再通过离线算法在数据集中进行“脑内”优化，超越人类示范的水平。
4.  **扩展到多机器人协作**：模块化控制和选择策略的思想可以扩展到多机器人系统，其中“选择”可能发生在机器人之间的任务分配和协调层面。

## 五、 总结与展望

论文《Coordinated Humanoid Manipulation with Choice Policies》为人形机器人的协调操控问题提供了一套务实而强大的解决方案。其贡献是双重的：
1.  **工程层面**：设计了**模块化遥操作接口**，破解了高质量人形机器人演示数据收集的瓶颈。
2.  **算法层面**：提出了**选择策略**，一种兼顾多峰性建模与快速推理的新型模仿学习架构，并在实证中超越了当前主流方法。

这项工作标志着人形机器人研究正从孤立的功能演示，迈向**系统化、可扩展、数据驱动的整体智能**。它揭示了一条清晰的技术路径：通过符合人类直觉的交互方式获取数据，通过精心设计的网络架构学习鲁棒策略，最终实现机器人在复杂、非结构化环境中像人一样自如地“看、想、动”。

随着硬件可靠性的提升和此类学习框架的成熟，我们有望在不久的将来，看到人形机器人真正走出实验室，在物流、家政、医疗辅助等场景中承担起需要全身协调的复杂任务，逐步融入人类的生产与生活。
