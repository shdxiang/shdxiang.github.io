---
title: "进化策略导致大型语言模型灾难性遗忘"
date: 2026-01-29 16:01:42 +0800
arxiv_id: 2601.20861v1
---

## 论文信息

**标题**: Evolutionary Strategies lead to Catastrophic Forgetting in LLMs

**作者**: Immanuel Abdi, Akshat Gupta, Micah Mok, et al.

**发布日期**: 2026-01-28

**arXiv ID**: [2601.20861v1](https://arxiv.org/abs/2601.20861v1)

**PDF链接**: [下载PDF](https://arxiv.org/pdf/2601.20861v1)

---


# 进化策略的“记忆诅咒”：大语言模型持续学习的阿喀琉斯之踵

## 论文背景与研究动机：持续学习的理想与现实鸿沟

当前人工智能系统最显著的缺陷之一，是缺乏部署后的持续学习能力。想象一下，如果人类在学习新知识时会完全遗忘旧技能，这样的学习方式显然是不可持续的。然而，这正是当前大语言模型（LLMs）面临的困境。

传统基于梯度的优化方法（如GRPO）虽然在大规模预训练中表现出色，但其内存需求巨大，难以支持模型的在线持续更新。每次微调都需要存储完整的梯度信息，导致计算成本和内存开销呈指数级增长。这种“静态”的学习范式严重限制了AI系统在动态环境中的适应能力。

正是在这样的背景下，进化策略（Evolutionary Strategies, ES）作为无梯度优化的代表重新进入研究视野。ES通过模拟自然选择过程，仅需评估候选解的性能而不计算梯度，理论上具有更低的内存开销和更好的并行性。然而，ES在持续学习场景中的表现如何？它是否真的能成为解决“灾难性遗忘”问题的银弹？这正是本论文探索的核心问题。

## 核心方法：进化策略的运作机制与技术细节

### 进化策略的基本原理

进化策略的核心思想借鉴了生物进化中的“变异-选择”机制。与基于梯度的方法直接沿着损失函数下降方向更新参数不同，ES通过以下步骤进行优化：

1. **种群生成**：在当前参数θ周围采样一组扰动向量{ε_i}，生成候选参数θ+σε_i
2. **适应度评估**：并行评估每个候选解在目标任务上的性能（适应度）
3. **权重更新**：根据适应度对扰动向量进行加权平均，更新模型参数

数学上，参数更新规则为：
θ_{t+1} = θ_t + α * (1/σN) * Σ_{i=1}^N F_i * ε_i

其中F_i是第i个候选解的适应度得分，α是学习率，σ是扰动尺度参数。

### 实验设计与对比基准

研究团队设计了严谨的实验框架：
- **模型架构**：使用标准Transformer架构的LLMs
- **任务设置**：数学推理和逻辑推理两类任务，模拟持续学习场景
- **对比算法**：GRPO（基于梯度的强化学习优化）作为主要对比基准
- **评估指标**：不仅关注新任务性能，更关键的是监测旧任务性能的衰减曲线

## 创新点与核心贡献：揭示无梯度优化的“记忆缺陷”

### 发现一：性能接近但代价巨大

研究首先证实了ES在特定任务上的竞争力。在相同的计算预算下，ES能够在数学推理任务上达到与GRPO相近的性能水平。这一发现本身具有重要意义，证明了无梯度方法在大规模语言模型优化中的可行性。

### 发现二：灾难性遗忘的量化证据

然而，研究的核心贡献在于揭示了ES在持续学习中的致命缺陷。通过系统性地增加更新步数并监测模型在先前任务上的表现，研究团队发现：

1. **遗忘曲线对比鲜明**：ES优化的模型表现出急剧的性能衰减，而GRPO模型则保持相对稳定的历史能力
2. **遗忘程度量化**：在某些任务上，ES模型在10次更新后，旧任务性能下降超过60%，而GRPO模型仅下降15-20%

### 发现三：遗忘机制的数学解释

研究进一步从数学角度解释了这种差异。通过分析参数更新的统计特性：

1. **更新稀疏性差异**：GRPO更新高度稀疏，只有少数关键参数发生显著变化；而ES更新几乎影响所有参数
2. **范数对比**：ES更新的ℓ2范数比GRPO大1-2个数量级，意味着更剧烈、更全局的参数扰动
3. **信息覆盖理论**：ES的全局扰动本质上“覆盖”了先前学习到的参数配置，导致历史记忆被系统性破坏

## 实验结果深度分析：数据背后的故事

### 数学推理任务的典型案例

在数学问题求解任务中，研究观察到一个有趣的现象：当模型学习新的数学技巧时，ES能够快速达到高准确率，但代价是完全丧失解决先前类型问题的能力。例如，模型在学习微积分问题后，可能完全忘记如何解决基本的代数方程。

### 多任务学习的扩展实验

研究还进行了多任务交替学习的实验。结果显示：
- **GRPO**：能够在一定程度上平衡不同任务，形成“任务间共享表示”
- **ES**：倾向于为每个任务创建独立的参数配置，导致严重的任务间干扰

### 计算效率的权衡分析

虽然ES在单次更新的内存开销较低，但其达到相同性能所需的更新次数通常是GRPO的3-5倍。在考虑总计算成本时，ES的优势并不明显，特别是在需要考虑历史性能保持的场景中。

## 实践应用建议：量化交易领域的启示

### 对算法交易系统的直接启示

1. **模型更新策略**：在开发自适应交易算法时，应谨慎使用ES类无梯度方法进行在线更新。建议采用“冻结核心+可塑外围”的架构设计，保护关键的市场模式识别能力。

2. **多市场适应**：对于需要在不同市场 regime 间切换的交易系统，GRPO类方法可能更适合，因为它们能更好地保持对历史市场状态的记忆。

3. **风险控制考量**：ES的灾难性遗忘特性可能带来不可预测的风险。在实盘部署前，必须进行全面的“遗忘压力测试”，评估模型在最坏情况下的性能衰减。

### 混合优化框架的设计思路

基于本研究的发现，可以设计分层优化框架：
- **底层参数**：使用GRPO进行稀疏、精准的微调
- **高层结构**：在适当约束下使用ES探索新的策略空间
- **记忆保护机制**：引入参数重要性加权，保护关键交易逻辑相关的参数

## 未来发展方向：解决遗忘问题的可能路径

### 技术改进方向

1. **稀疏进化策略**：开发能够产生稀疏更新的ES变体，结合进化策略的并行优势和梯度方法的精准性
2. **约束进化优化**：在ES更新中引入参数变化约束，限制每次更新的最大扰动范围
3. **模块化架构**：设计显式分离的模块化网络，不同模块负责不同能力，减少干扰

### 理论探索方向

1. **遗忘动力学的数学建模**：建立更精确的理论框架，预测不同优化算法的遗忘行为
2. **容量-稳定性权衡**：深入研究神经网络表示容量与记忆稳定性之间的根本权衡
3. **生物启发的学习机制**：从神经科学中汲取灵感，模拟人脑的睡眠、回放等记忆巩固机制

### 评估标准创新

需要建立更全面的持续学习评估基准，包括：
- 长期记忆保持度
- 正向/负向迁移的量化指标
- 计算效率与记忆效率的联合优化目标

## 总结与展望：持续学习之路任重道远

本研究的价值不仅在于揭示了ES在持续学习中的局限性，更在于它促使我们重新思考AI系统学习的基本范式。灾难性遗忘不是ES特有的问题，而是所有机器学习系统在追求适应性时必须面对的 fundamental trade-off。

### 关键启示

1. **没有免费的午餐**：ES在并行性和内存效率上的优势，是以牺牲记忆稳定性为代价的
2. **梯度信息的重要性**：梯度不仅提供了优化方向，还包含了参数重要性的隐含信息
3. **持续学习的复杂性**：真正的持续学习需要平衡稳定性与可塑性，这不是单一算法能解决的问题

### 长期展望

未来的持续学习系统可能需要融合多种范式：
- **神经符号结合**：将稳定的符号表示与可塑的神经表示相结合
- **终身学习架构**：设计专门支持增量学习的网络架构和训练协议
- **元学习框架**：让模型学会如何学习，自适应地调整自己的学习策略

这项研究为理解优化算法与记忆保持之间的关系提供了重要洞见。它提醒我们，在追求AI系统更强大适应能力的同时，不能忽视其保持已有能力的基本需求。只有在这两者之间找到恰当的平衡，我们才能真正迈向能够持续学习、不断进化的人工智能系统。

正如论文作者所期望的，这项工作应该成为未来研究的起点，激励更多研究者探索缓解灾难性遗忘的创新方法，最终实现真正意义上的持续学习AI系统。
