---
title: "生成式视图拼接"
date: 2025-10-29 16:03:02 +0800
arxiv_id: 2510.24718v1
---

## 论文信息

**标题**: Generative View Stitching

**作者**: Chonghyuk Song, Michal Stary, Boyuan Chen, et al.

**发布日期**: 2025-10-28

**arXiv ID**: [2510.24718v1](https://arxiv.org/abs/2510.24718v1)

**PDF链接**: [下载PDF](https://arxiv.org/pdf/2510.24718v1)

---


# 生成式视角缝合：突破自回归视频生成的时序限制

## 论文背景与研究动机

在当今数字内容创作蓬勃发展的时代，视频生成技术正成为计算机视觉和图形学领域的热点研究方向。特别是随着扩散模型在图像生成领域取得的巨大成功，研究者们开始将这一技术扩展到视频生成领域。然而，视频生成面临着比图像生成更为复杂的挑战——不仅需要保证单帧的质量，还需要维持帧与帧之间的时序一致性和视觉连贯性。

传统的自回归视频扩散模型虽然在生成长序列时能够保持稳定性和与历史帧的一致性，但它们存在一个根本性限制：无法利用未来信息来指导当前帧的生成。这一限制在相机引导的视频生成任务中尤为明显。当使用预定义的相机轨迹生成视频时，模型由于无法预见未来的相机运动，常常会导致生成的场景与相机轨迹发生"碰撞"，进而引发整个生成过程的崩溃。

这一问题的根源在于自回归生成的基本范式：模型只能基于过去的信息生成当前帧，而对未来的相机位置和视角变化一无所知。这就好比一个人在迷宫中行走，只能看到已经走过的路，而无法预知前方的转弯，因此很容易走入死胡同。在视频生成中，这种"短视"行为会导致场景几何与相机轨迹不匹配，最终造成视觉上的不连贯和生成失败。

论文作者团队观察到，在机器人路径规划领域已经存在类似的挑战，并且已经发展出一些解决方案。受此启发，他们开始思考：是否能够将机器人规划中的"缝合"技术迁移到视频生成领域，从而解决相机引导视频生成中的碰撞问题？这一研究动机催生了Generative View Stitching（GVS）方法的诞生。

## 核心方法和技术细节

### 基础框架：Diffusion Forcing

GVS方法建立在一个称为"Diffusion Forcing"的序列扩散框架之上，这是当前视频生成领域的主流框架之一。该框架的关键insight在于：在训练过程中，模型不仅学习从噪声生成视频帧，还学习在给定部分观测帧的条件下生成完整的视频序列。这种训练方式意外地为后续的"缝合"操作提供了必要的基础设施。

Diffusion Forcing的核心思想是通过在训练过程中随机掩码部分视频帧，强制模型学习基于上下文信息（包括过去和未来的帧）来重建被掩码的帧。这种训练策略使得模型具备了"填空"的能力，即根据周围的信息来推测缺失部分的内容。

### 生成式视角缝合算法

GVS的核心贡献是一个新颖的采样算法，该算法扩展了之前在机器人规划中使用的扩散缝合技术，使其适用于视频生成任务。与传统的自回归生成不同，GVS采用并行采样的方式处理整个视频序列，确保生成的场景忠实于预定义相机轨迹的每个部分。

技术实现上，GVS算法包含以下几个关键步骤：

1. **全局优化采样**：不同于自回归方法的局部贪婪采样，GVS同时考虑整个时间序列上的所有帧，通过迭代优化使得生成的视频在任意时间点都与相机轨迹保持一致。

2. **双向条件化**：GVS利用Diffusion Forcing框架提供的特性，在生成每个帧时同时考虑过去和未来的信息。这种双向条件化确保了时间上的连贯性，避免了传统方法中因只考虑历史信息而导致的累积误差。

3. **循环闭合机制**：针对长序列生成中的漂移问题，GVS引入了循环闭合机制。当相机轨迹形成闭环时（如返回到起始点），该机制确保起始帧和结束帧之间的无缝衔接，维持长距离的一致性。

### 全向指导技术

论文中提出的"Omni Guidance"技术是GVS方法的重要组成部分。该技术通过同时条件化过去和未来的信息，显著增强了缝合过程中的时间一致性。具体而言，Omni Guidance在扩散过程的每个去噪步骤中，不仅考虑之前已生成的帧，还考虑之后将要生成的帧（根据预定义的相机轨迹），从而做出更加全局一致的生成决策。

这种双向条件化的实现依赖于Diffusion Forcing训练出的模型的内在特性。在训练过程中，模型已经学会了如何基于不完整的观测（可能包括过去帧、未来帧或两者的组合）来生成缺失的帧。在推理阶段，GVS利用这一能力，在生成每个帧时提供最相关的上下文信息，无论是来自过去还是未来。

## 创新点和贡献

GVS方法的主要创新点和贡献可以总结为以下几个方面：

### 1. 并行采样范式的转变

GVS最显著的创新是将视频生成从传统的自回归序列生成转变为全局并行采样。这一转变从根本上解决了自回归方法无法利用未来信息的问题，使得生成的视频能够与完整的相机轨迹保持一致。

### 2. 模型无关的缝合框架

与通常需要专门训练模型的缝合方法不同，GVS与任何使用Diffusion Forcing训练的现成视频模型兼容。这一特性大大提高了方法的实用性和可扩展性，使得研究者可以在不重新训练模型的情况下应用GVS技术。

### 3. 全向指导与循环闭合

Omni Guidance技术的引入使得模型能够同时考虑过去和未来的上下文信息，显著提升了时间一致性。而循环闭合机制则解决了长序列生成中的漂移问题，确保了闭环轨迹下的视觉连贯性。

### 4. 理论洞察与实践创新的结合

论文的一个重要理论贡献是指出并证明了Diffusion Forcing训练框架已经天然地提供了缝合所需的特性。这一洞察不仅解释了为什么GVS可以与现成模型兼容，也为理解序列扩散模型的内部工作机制提供了新的视角。

## 实验结果分析

根据论文中报告的结果，GVS在多种预定义相机路径上均表现出色，包括具有挑战性的"奥斯卡·路特斯瓦尔德不可能楼梯"这样的复杂轨迹。与基线方法相比，GVS生成的视频在以下几个关键指标上均有显著提升：

### 稳定性与碰撞避免

实验结果显示，GVS能够有效避免与生成场景的碰撞，而传统的自回归方法在经过一定帧数后都会出现碰撞并导致生成崩溃。这一改进使得生成长时间、复杂相机运动视频成为可能。

### 帧间一致性

通过定量和定性分析，GVS在帧间一致性指标上明显优于自回归基线。这主要归功于Omni Guidance提供的双向条件化，使得每个帧的生成都考虑了全局上下文。

### 循环闭合能力

在包含循环的相机路径测试中，GVS展现出了卓越的循环闭合能力，能够无缝连接循环的起点和终点。而基线方法在循环路径上通常会出现明显的跳变和不连贯。

### 复杂轨迹处理

对于像"不可能楼梯"这样包含视觉悖论的复杂轨迹，GVS能够生成视觉上合理的视频，尽管相机的运动在物理上是不可能的。这展示了方法在处理反直觉几何结构方面的强大能力。

论文作者提供了项目网站（https://andrewsonga.github.io/gvs），建议通过观看视频结果来全面评估方法的性能，因为动态视频比静态图像更能展示时间一致性和流畅性。

## 实践应用建议和未来发展方向

### 实践应用建议

基于GVS的技术特点，我们提出以下实践应用建议：

1. **虚拟制片与预可视化**：GVS可以用于电影和游戏制作中的预可视化阶段，允许导演和摄影师在实际拍摄前预览复杂的相机运动效果，节省制作成本和时间。

2. **虚拟现实与增强现实**：在VR/AR应用中，GVS可以生成与用户头部运动一致的动态环境，提供更加沉浸式的体验。

3. **机器人导航模拟**：可以将GVS应用于机器人训练环境的生成，创建与预定路径一致的视觉场景，加速机器人的视觉导航学习过程。

4. **建筑与空间设计**：设计师可以使用GVS生成沿着预定路径的建筑漫游视频，更好地展示设计概念和空间关系。

### 技术实施考量

在实施GVS时，需要注意以下几点：

1. **计算资源需求**：并行采样整个序列相比自回归方法需要更多的显存，特别是在生成长序列时。需要根据可用硬件调整序列长度或采用分段处理策略。

2. **相机轨迹设计**：GVS严重依赖预定义的相机轨迹质量。轨迹应该平滑且物理合理，以避免生成不自然的视觉效果。

3. **模型选择**：虽然GVS与任何Diffusion Forcing训练的模型兼容，但基础模型的质量直接影响最终结果。选择在特定领域表现良好的预训练模型至关重要。

### 未来发展方向

基于GVS的当前局限性和潜在可能性，我们提出以下未来研究方向：

1. **实时生成优化**：当前的GVS方法计算成本较高，未来的工作可以专注于优化采样效率，实现实时或近实时的生成速度。

2. **自适应轨迹规划**：结合GVS与相机轨迹优化算法，开发能够根据场景内容自动调整相机路径的系统。

3. **多模态条件化**：扩展GVS以支持文本、音频等多模态条件输入，实现更加可控和多样化的视频生成。

4. **3D场景理解集成**：将显式的3D场景表示与GVS结合，进一步提高生成视频的几何一致性和物理合理性。

5. **长序列生成优化**：开发分层或分段策略，克服内存限制，实现极长序列的生成。

## 总结与展望

Generative View Stitching代表了视频生成领域的一个重要进步，通过将机器人规划中的缝合概念引入视频生成，巧妙地解决了自回归方法无法利用未来信息的关键限制。GVS的核心优势在于其并行采样范式、与现成模型的兼容性以及通过Omni Guidance实现的双向条件化。

这一工作的意义不仅在于其技术贡献，更在于它展示了不同领域（机器人学和计算机视觉）之间概念迁移的价值。GVS的成功表明，解决复杂问题有时需要跳出本领域的常规思维，从其他领域寻找灵感。

从更广阔的视角看，GVS是朝着更加智能、可控的视频生成系统迈出的重要一步。随着技术的进一步发展，我们有望看到视频生成模型在创造性表达、虚拟环境构建和模拟系统等领域发挥更加重要的作用。

未来的视频生成研究可能会继续朝着几个关键方向发展：更高的生成质量与一致性、更强的可控性与交互性、更高的计算效率，以及更好的与3D世界模型的集成。GVS在这些方向上都提供了有价值的思路和方法论启示。

总之，Generative View Stitching不仅解决了相机引导视频生成中的具体技术挑战，更为序列生成领域提供了新的思考框架，有望启发更多创新工作的出现，推动整个领域向前发展。
