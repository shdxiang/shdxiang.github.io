---
title: "扩展验证比扩展策略学习更能有效实现视觉-语言-动作对齐"
date: 2026-02-13 16:01:31 +0800
arxiv_id: 2602.12281v1
---

## 论文信息

**标题**: Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment

**作者**: Jacky Kwok, Xilun Zhang, Mengdi Xu, et al.

**发布日期**: 2026-02-12

**arXiv ID**: [2602.12281v1](https://arxiv.org/abs/2602.12281v1)

**PDF链接**: [下载PDF](https://arxiv.org/pdf/2602.12281v1)

---


# 缩小“意图-行动鸿沟”：验证比策略学习更有效？——CoVer框架深度解析

## 论文背景与研究动机

通用机器人技术的长期愿景，始终围绕着其理解和执行自然语言指令的能力展开。近年来，**视觉-语言-行动模型** 在这一领域取得了显著进展，它们能够将视觉感知、语言理解与物理动作生成相结合。然而，一个根本性的挑战依然存在：模型生成的动作可能与给定的指令**意图**产生偏差，即所谓的 **“意图-行动鸿沟”**。

传统的解决思路主要聚焦于**扩大策略学习**的规模，即通过海量的数据和计算资源，在训练阶段让模型学习更精确的指令-动作映射。然而，这种方法成本高昂，且在面对**分布外**或**长尾**指令时，性能提升往往遭遇瓶颈。

本文作者敏锐地洞察到，与其在训练阶段投入巨量资源，不如在**测试/部署阶段**（即模型实际执行任务时）引入一个动态的、可扩展的**验证机制**。其核心研究动机在于：**通过测试时验证，利用计算资源对模型生成的候选动作进行筛选和修正，是否比单纯扩大预训练规模更能高效、低成本地弥合意图-行动鸿沟？**

## 核心方法：CoVer框架与技术细节

本文提出的解决方案名为 **CoVer**，这是一个**对比验证器**框架，其核心思想是将“生成-验证”范式系统化，并利用测试时的计算资源进行大规模采样和择优。

### 1. 测试时扩展定律的发现
论文首先通过实验揭示了一个关键现象：在测试时，**联合扩展指令重述数量和生成动作数量**，能极大地增加候选样本的多样性，从而比单独扩展任一维度更有效地恢复出正确的动作。这为后续的验证方法提供了理论基础——**多样性是提升验证效果的关键**。

### 2. CoVer架构与“启动时计算”
CoVer框架的核心流程分为两个阶段：
*   **启动时计算**：在机器人部署并开始执行具体任务指令**之前**，系统会预先进行一轮计算。具体步骤包括：
    1.  **指令多样化**：利用一个**视觉-语言模型** 对原始指令生成多个语义等价但表述不同的**重述指令**。这解决了语言表达的歧义性和多样性问题。
    2.  **动作候选生成**：针对每一条重述指令，VLA策略模型会**重复生成多个动作序列候选**。这形成了一个庞大的、多样化的“指令-动作对”候选池。
*   **分层验证推理管道**：在拥有候选池后，CoVer使用一个训练好的**对比验证器** 进行择优。这个过程是分层的：
    1.  **高层提示选择**：验证器首先评估哪一条重述指令最能准确反映原始意图，并筛选出最优的**高层语义提示**。
    2.  **低层动作块选择**：在选定的最优指令下，验证器进一步从多个生成的动作序列中，挑选出与当前视觉场景最匹配、最可能成功执行的低层**动作块**。

### 3. 对比验证器的训练
验证器本身是一个神经网络，其训练目标是学习一个评分函数。给定一个**三元组**—— 视觉观察、语言指令、候选动作序列，验证器需要判断该动作序列成功执行指令的可能性。训练数据通常来源于策略模型在仿真环境中的成功与失败轨迹，通过**对比学习**的方式，让模型学会区分高质量与低质量的动作序列。

**技术精髓**在于，CoVer将问题从“一次性生成完美动作”转变为“生成多个可能选项，并利用一个轻量级但精准的判别器（验证器）进行选择”。这降低了对策略模型生成精度的绝对要求，转而依靠验证器的判别能力。

## 创新点与主要贡献

1.  **范式转换**：提出了从“缩放预训练”到“缩放验证”的范式转变。论证了在测试/部署阶段投入计算资源进行验证，可以比单纯扩大训练数据规模更**高效、更具成本效益**地提升性能。
2.  **系统性验证框架**：首次提出了一个完整的、包含“启动时计算”和“分层验证”的VLA测试时验证框架（CoVer），并证明了其卓越的可扩展性。
3.  **测试时扩展定律**：实证发现了测试时联合扩展指令与动作维度对提升性能的关键作用，为后续研究提供了新的缩放方向。
4.  **显著的性能提升**：在多个基准测试上取得了突破性成果，尤其是在**真实世界**实验中表现出的巨大优势，证明了该方法的实用价值。

## 实验结果分析

论文在**SIMPLER**和**PolaRiS**两个具身指令跟随基准上进行了全面评估，结果极具说服力：

*   **对比基线**：与使用**相同数据量**进行策略预训练缩放的方法相比，CoVer验证方法在SIMPLER基准上取得了**22%** 的分布内性能提升和**13%** 的分布外性能提升。这直接证明了“缩放验证”比“缩放策略学习”更有效。
*   **真实世界验证**：在真实的机器人实验场景中，CoVer带来了**额外45%** 的性能改进。这一结果至关重要，它表明该方法能有效应对仿真到实物的转移差距和现实世界中的大量不确定性。
*   **任务进度与成功率**：在PolaRiS基准上，CoVer在**任务进度**指标上提升14%，在**最终成功率**上提升9%。这说明验证不仅帮助机器人更正确地开始任务，还能引导其更连贯、更完整地执行整个任务流程。

这些实验数据强有力地支撑了论文的核心论点：**对于弥合意图-行动鸿沟，投资于测试时的智能验证比投资于训练时的数据堆砌更具性价比和效果。**

## 实践应用建议与未来方向

### 对机器人及VLA领域的实践建议：
1.  **系统设计思路**：在构建实用的服务或工业机器人系统时，应考虑将“生成-验证”作为核心架构。可以部署一个相对轻量、快速的生成模型，搭配一个经过精心训练、判别力强的验证模块。
2.  **计算资源分配**：重新评估计算资源的分配策略。在预算有限的情况下，可以考虑将部分从大规模预训练中节省的资源，转移到部署端的验证计算上，以获取更直接的性能回报。
3.  **安全性与可靠性**：验证机制天然适合用于**安全关键**场景。通过让验证器对危险或不确定的动作序列给出低分，可以构建一道安全防线，增强机器人系统的可靠性和可信度。

### 未来研究方向：
1.  **验证器效率优化**：当前方法需要生成大量候选，计算开销依然存在。未来可研究更高效的候选生成策略（如基于不确定性的采样）和更快速的验证器架构。
2.  **在线与持续验证**：目前的验证主要在动作执行前进行（前验）。未来可探索**在线验证**，即在动作执行过程中持续监控状态，并能进行实时修正或重规划。
3.  **多模态验证**：除了动作序列，是否可以引入更多反馈信号进行验证？例如，结合力觉、触觉等多模态传感信息，或预测动作执行后的预期视觉状态，进行更全面的前瞻性验证。
4.  **与基础模型结合**：如何将CoVer框架与强大的**视觉-语言基础模型** 更深度地结合？例如，直接利用VLM的内在知识进行零样本或小样本的验证，减少对特定任务验证数据的需求。

## 总结与展望

本文《Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment》是一篇在具身人工智能领域具有重要启发意义的工作。它挑战了“更大规模预训练=更好性能”的惯性思维，通过严谨的实验和创新的框架设计，证明了**测试时验证是一种被低估的强大工具**。

CoVer框架的成功，不仅仅在于其带来的性能提升，更在于它开辟了一条提升AI系统可靠性和对齐能力的新路径。它告诉我们，智能不仅体现在“生成”的能力上，也体现在“判断”和“选择”的能力上。在通往通用机器人的道路上，如何让机器人在复杂、开放的世界中做出稳健、符合人类意图的决策，CoVer所代表的“验证优于生成”的哲学，或许将成为下一代AI系统设计的关键范式之一。

未来，随着计算资源的持续增长和基础模型的不断进化，如何将生成与验证更优雅、更高效地融合，构建出既能创造性思考又能审慎判断的智能体，将是学术界和工业界共同面临的激动人心的挑战。
