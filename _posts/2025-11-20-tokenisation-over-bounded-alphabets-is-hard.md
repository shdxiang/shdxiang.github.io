---
title: "有界字母表上的分词是困难的"
date: 2025-11-20 16:01:55 +0800
arxiv_id: 2511.15709v1
---

## 论文信息

**标题**: Tokenisation over Bounded Alphabets is Hard

**作者**: Violeta Kastreva, Philip Whittington, Dennis Komm, et al.

**发布日期**: 2025-11-19

**arXiv ID**: [2511.15709v1](https://arxiv.org/abs/2511.15709v1)

**PDF链接**: [下载PDF](https://arxiv.org/pdf/2511.15709v1)

---


# 有限字母表上的分词问题是计算难题：理论突破与实践启示

## 论文背景与研究动机

在自然语言处理领域，分词（tokenisation）是一个基础而关键的预处理步骤，它将输入文本分割成有意义的单元（tokens）。近年来，随着BERT、GPT等大型语言模型的兴起，分词技术的重要性愈发凸显。然而，一个长期被忽视的理论问题逐渐浮出水面：分词的计算复杂度究竟如何？

此前的研究已经证明，在**无限大字母表**的假设下，分词问题是NP完全的。这意味着，随着输入规模的增大，找到最优分词方案所需的时间呈指数级增长。然而，这一假设与现实严重脱节——实际应用中的分词器都运行在**固定大小的字母表**上，如字节（256个字符）或Unicode字符集。

这种理论与实践的脱节促使研究者重新审视分词问题的计算本质。如果分词在有限字母表上仍然是计算难题，那么这将解释为什么实际分词算法（如BPE、WordPiece）都采用启发式方法而非寻求最优解。本研究正是为了填补这一理论空白，探索在有限字母表条件下分词问题的计算复杂度。

## 核心方法和技术细节

### 问题定义与变体

论文研究了两种自然的分词问题变体：

1. **自底向上分词（Bottom-up Tokenisation）**：从基础字符开始，通过一系列合并操作构建分词结果，类似于BPE（Byte Pair Encoding）算法的过程。

2. 直接分词（Direct Tokenisation）：直接选择一个词汇表，然后应用该词汇表对数据集进行最优压缩。

### 关键技术洞察

论文首先提出了一个关键观察：**证明n元字母表上的难度结果，等同于证明了任意更大字母表上的相同结果**。这一简化使得研究者可以专注于最小规模的字母表——二元字母表（binary alphabet）和一元字母表（unary alphabet）。

### 复杂度证明框架

研究者采用了精妙的**归约（reduction）** 技术，将已知的NP完全问题映射到分词问题上：

- 对于二元字母表，论文证明了两种分词变体不仅是NP完全的，而且**不存在多项式时间近似方案（PTAS）**，除非P=NP。这意味着，即使是寻找近似最优解，在理论上也是极其困难的。

- 对于一元字母表，论文证明了直接分词仍然是NP完全的。这一结果尤其令人惊讶，因为一元字母表看似简单——仅包含一个符号。这表明分词的**计算难度不是来自大字母表或复杂构造，而是问题的内在本质**。

证明过程中，研究者将经典的**集合覆盖问题**和**图着色问题**巧妙地转化为分词问题，建立了计算复杂度上的等价关系。这种归约保证了如果能在多项式时间内解决分词问题，就能同样解决那些已知的NP完全问题，这在计算复杂性理论中是不可能的（除非P=NP）。

## 创新点与理论贡献

### 理论突破

1. **填补理论空白**：首次系统分析了有限字母表条件下的分词复杂度，弥合了理论与实践的差距。

2. **强硬度结果**：不仅证明了NP完全性，还证明了不存在多项式时间近似方案，这比单纯的NP完全性结果更强。

3. **最小字母表分析**：通过分析二元和一元字母表，揭示了分词问题的**内在计算难度**，证明这种难度不是人为构造的产物。

### 对实践的解释力

论文的理论结果解释了为什么实际分词算法都是启发式的：

- **BPE（字节对编码）** 采用贪心策略，逐步合并最频繁的字符对
- **Unigram语言模型** 基于概率模型和动态规划寻找近似最优解
- **WordPiece** 同样基于贪心策略和似然最大化

这些算法都**放弃了寻找全局最优解**，转而寻求在合理时间内获得高质量近似解，这正好与理论上的硬度结果相吻合。

## 实验结果的理论意义

虽然论文主要关注理论分析，但其结果对实验研究具有重要指导意义：

1. **算法性能极限**：理论结果解释了为什么不同分词算法在不同数据集上表现不一致——寻找全局最优解在计算上不可行。

2. **评估标准反思**：既然最优解不可得，评估分词算法时应该更加关注**实用性能**而非与理论最优的差距。

3. **资源分配指导**：研究结果为算法设计中的时间-质量权衡提供了理论依据，指导研究者合理分配计算资源。

## 实践应用建议

### 对量化交易的启示

在量化交易中，文本分析同样面临分词挑战：

1. **金融新闻情感分析**：应采用**增量更新**的分词策略，而非每次都重新计算最优分词。

2. **算法适应性**：开发能够适应市场环境变化的动态分词算法，避免追求静态最优。

3. **计算资源分配**：将更多资源分配给下游任务（如情感分类、关系提取），而非过度优化分词本身。

### 对人工智能领域的建议

1. **拥抱启发式方法**：接受最优解的不可得性，专注于改进启发式算法的**稳定性和泛化能力**。

2. **多目标优化**：将分词与其他NLP任务联合优化，而非孤立地优化分词本身。

3. **领域自适应**：开发能够快速适应新领域的分词算法，减少对全局最优的依赖。

### 具体技术方向

1. **近似算法研究**：开发具有**理论保证的近似算法**，如固定参数可处理算法或近似比有界的算法。

2. **学习型分词器**：利用强化学习或元学习框架，让模型学会在不同上下文中选择合适的分词策略。

3. **硬件加速**：针对特定分词算法设计专用硬件，通过并行计算缓解计算复杂度问题。

## 总结与展望

本研究通过严谨的理论分析，确立了有限字母表上分词问题的计算难度，为理解实际分词算法的局限性提供了理论基础。论文的核心贡献在于证明了即使是最简单的字母表（二元、一元）条件下，分词问题仍然是NP完全的，且难以有效近似。

### 理论意义总结

1. 分词的计算难度是**内在的、根本的**，而非特定假设的产物。
2. 理论结果解释了为什么**启发式方法**在实践中占据主导地位。
3. 研究为分词算法的**性能极限**划定了明确边界。

### 未来研究方向

1. **近似算法理论**：探索在弱化条件下（如限制词汇表大小、输入结构）的可近似性。

2. **学习理论框架**：将分词问题置于在线学习或强化学习框架下，研究其遗憾界和样本复杂度。

3. **跨模态分词**：将理论分析扩展到多模态场景，如图文混合内容的分词问题。

4. **量子计算视角**：探索量子算法在分词问题上的潜力，研究是否存在量子加速的可能性。

这项研究为分词领域奠定了坚实的理论基础，同时也指明了实践发展的方向：与其追求理论上最优但计算不可行的算法，不如专注于开发高效、稳定、自适应的近似算法。这一思路不仅适用于分词问题，也对整个自然语言处理领域具有启发意义。

在人工智能快速发展的今天，理解基础问题的计算本质比以往任何时候都更加重要。只有深入认识问题的根本限制，我们才能制定合理的研究目标，开发出既实用又有理论依据的算法系统。
